"""
Fake response generator cho testing - Cline XML Format
"""
import uuid
import time
import re
from typing import List, Any, Optional, Dict, Tuple

def _is_plan_mode(messages: List[Any]) -> bool:
    """Ki·ªÉm tra xem Cline c√≥ ƒëang ·ªü PLAN MODE kh√¥ng"""
    if not messages:
        return False
        
    for msg in messages:
        if hasattr(msg, 'content'):
            content = msg.content
        elif hasattr(msg, 'get'):
            content = msg.get('content', '')
        else:
            continue
            
        # Ki·ªÉm tra trong environment_details
        if isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and item.get('type') == 'text':
                    text = item.get('text', '')
                    if 'Current Mode\nPLAN MODE' in text:
                        return True
        elif isinstance(content, str) and 'Current Mode\nPLAN MODE' in content:
            return True
            
    return False

def create_fake_response(model: str = "deepseek-chat", messages: List[Any] = None, stream: bool = False) -> dict:
    """
    T·∫°o fake response chu·∫©n OpenAI ƒë·ªÉ test backend m√† kh√¥ng c·∫ßn g·ªçi DeepSeek th·ª±c s·ª±.
    H·ªó tr·ª£ c·∫£ streaming v√† non-streaming responses.
    """
    # Ki·ªÉm tra xem c√≥ ƒëang ·ªü PLAN MODE kh√¥ng
    is_plan_mode = _is_plan_mode(messages)
    
    if is_plan_mode:
        return _create_plan_mode_response(model, messages, stream)
    
    # ∆ØU TI√äN: Ki·ªÉm tra <task> tag tr∆∞·ªõc
    if messages and _contains_task_tag(messages):
        task_content = _extract_task_content(messages)
        # S·ª≠ d·ª•ng response chu·∫©n cho c√°c task ƒë∆°n gi·∫£n
        if any(simple_task in task_content.lower() for simple_task in ['hello', 'hi', 'xin ch√†o', 'test']):
            return _create_attempt_completion_response(model, task_content, stream)
        else:
            return _create_task_response_with_xml(model, messages, stream)
    # Ki·ªÉm tra n·∫øu c√≥ messages v√† c√≥ ch·ª©a tool use pattern
    elif messages and _contains_tool_use_pattern(messages):
        return _create_xml_tool_call_response(model, stream)
    else:
        return _create_standard_response(model, stream)


def _contains_task_tag(messages: List[Any]) -> bool:
    """Ki·ªÉm tra n·∫øu messages c√≥ ch·ª©a <task> tag trong user message"""
    for msg in messages:
        # X·ª≠ l√Ω c·∫£ object v√† dict
        if hasattr(msg, 'role'):
            role = msg.role
            content = msg.content
        elif hasattr(msg, 'get'):
            role = msg.get('role')
            content = msg.get('content', '')
        else:
            continue

        if role != "user":
            continue

        # X·ª≠ l√Ω content d·∫°ng list (multimodal)
        if isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and item.get('type') == 'text':
                    if '<task>' in item.get('text', '').lower():
                        return True
        elif isinstance(content, str) and '<task>' in content.lower():
            return True
    
    return False

def _contains_tool_use_pattern(messages: List[Any]) -> bool:
    """Ki·ªÉm tra n·∫øu messages c√≥ ch·ª©a pattern s·ª≠ d·ª•ng tool"""
    for msg in messages:
        # X·ª≠ l√Ω c·∫£ object v√† dict
        if hasattr(msg, 'role'):
            role = msg.role
            content = msg.content
        elif hasattr(msg, 'get'):
            role = msg.get('role')
            content = msg.get('content', '')
        else:
            continue
        
        # Ki·ªÉm tra n·∫øu c√≥ system message v·ªõi TOOL USE
        if role == "system" and "TOOL USE" in str(content):
            return True
    
    return False

def _create_task_response_with_xml(model: str, messages: List[Any], stream: bool = False) -> dict:
    """
    T·∫°o response v·ªõi XML khi nh·∫≠n ƒë∆∞·ª£c <task> t·ª´ user (Cline c·∫ßn XML)
    H·ªó tr·ª£ c·∫£ streaming v√† non-streaming
    """
    task_content = _extract_task_content(messages)
    
    # T·∫°o XML content d·ª±a tr√™n n·ªôi dung task
    if "hello" in task_content.lower():
        xml_content = """<thinking>
User sent a greeting "hello". I'll respond with attempt_completion and create a simple task progress.
</thinking>

<attempt_completion>
<result>Hello! I've received your greeting and I'm ready to assist you with the ZenEnd project. Please let me know your specific requirements.</result>
<task_progress>
- [x] Acknowledge user greeting
- [ ] Wait for specific task requirements
</task_progress>
</attempt_completion>"""
    elif "test" in task_content.lower():
        xml_content = """<thinking>
The user is testing the API. I should examine the project structure to understand how it works.
</thinking>

<read_file>
<path>main.py</path>
<task_progress>
- [ ] Analyze main application structure
- [ ] Review API routes implementation  
- [ ] Test API functionality
- [ ] Verify response format
</task_progress>
</read_file>"""
    elif "api" in task_content.lower() or "route" in task_content.lower():
        xml_content = """<thinking>
The user wants to understand API routes. I should examine the API implementation.
</thinking>

<read_file>
<path>api/routes.py</path>
<task_progress>
- [ ] Analyze API routes structure
- [ ] Review request handling
- [ ] Check response formatting
- [ ] Test endpoint functionality
</task_progress>
</read_file>"""
    elif "websocket" in task_content.lower() or "socket" in task_content.lower():
        xml_content = """<thinking>
The user is interested in WebSocket functionality. I should examine the WebSocket implementation.
</thinking>

<read_file>
<path>websocket/server.py</path>
<task_progress>
- [ ] Analyze WebSocket server
- [ ] Review connection handling
- [ ] Check message processing
- [ ] Test real-time functionality
</task_progress>
</read_file>"""
    elif "fake" in task_content.lower():
        xml_content = """<thinking>
The user wants to understand the fake response system. I should examine the current implementation.
</thinking>

<read_file>
<path>api/fake_response.py</path>
<task_progress>
- [ ] Analyze fake response system
- [ ] Review response generation
- [ ] Check test scenarios
- [ ] Improve fake response logic
</task_progress>
</read_file>"""
    else:
        xml_content = f"""<thinking>
The user wants me to work on: {task_content}. I need to understand the project structure first.
</thinking>

<list_files>
<path>.</path>
<recursive>true</recursive>
<task_progress>
- [ ] Analyze task requirements: {task_content}
- [ ] Explore project structure
- [ ] Identify relevant files and components
- [ ] Develop implementation strategy
</task_progress>
</list_files>"""

    if stream:
        return _create_streaming_response(model, xml_content)
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": xml_content,
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": _calculate_tokens(messages),
                "completion_tokens": _count_tokens(xml_content),
                "total_tokens": _calculate_tokens(messages) + _count_tokens(xml_content)
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }

def _create_attempt_completion_response(model: str, task_content: str, stream: bool = False) -> dict:
    """
    T·∫°o response v·ªõi attempt_completion cho Cline (c·∫ßn XML format)
    H·ªó tr·ª£ c·∫£ streaming v√† non-streaming
    """
    xml_content = f"""<thinking>
Ng∆∞·ªùi d√πng ƒë√£ g·ª≠i task: "{task_content}". V√¨ ƒë√¢y l√† m·ªôt task ƒë∆°n gi·∫£n kh√¥ng ƒë√≤i h·ªèi code c·ª• th·ªÉ, t√¥i s·∫Ω d√πng attempt_completion ƒë·ªÉ ph·∫£n h·ªìi v√† ƒë·ªìng th·ªùi t·∫°o m·ªôt task_progress list ƒë∆°n gi·∫£n cho c√°c b∆∞·ªõc ti·∫øp theo.
</thinking>

<attempt_completion>
<result>I've received your task: "{task_content}". I'm currently in fake response mode and ready to assist you with the ZenEnd project. Please provide more specific requirements.</result>
<task_progress>
- [x] Receive and acknowledge user task
- [ ] Wait for specific requirements
</task_progress>
</attempt_completion>"""

    if stream:
        return _create_streaming_response(model, xml_content)
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": xml_content,
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": _calculate_tokens([]),
                "completion_tokens": _count_tokens(xml_content),
                "total_tokens": _calculate_tokens([]) + _count_tokens(xml_content)
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }

def _extract_task_content(messages: List[Any]) -> str:
    """Tr√≠ch xu·∫•t n·ªôi dung t·ª´ <task> tag"""
    for msg in messages:
        # X·ª≠ l√Ω c·∫£ object v√† dict
        if hasattr(msg, 'role'):
            role = msg.role
            content = msg.content
        elif hasattr(msg, 'get'):
            role = msg.get('role')
            content = msg.get('content', '')
        else:
            continue

        if role != "user":
            continue

        # X·ª≠ l√Ω content d·∫°ng list (multimodal)
        if isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and item.get('type') == 'text':
                    text_content = item.get('text', '')
                    task_content = _extract_task_from_text(text_content)
                    if task_content:
                        return task_content
        elif isinstance(content, str):
            task_content = _extract_task_from_text(content)
            if task_content:
                return task_content
    
    return "Unknown task"


def _extract_task_from_text(text: str) -> str:
    """Tr√≠ch xu·∫•t n·ªôi dung task t·ª´ text"""
    # T√¨m n·ªôi dung gi·ªØa <task> tags
    task_match = re.search(r'<task>(.*?)</task>', text, re.IGNORECASE | re.DOTALL)
    if task_match:
        return task_match.group(1).strip()
    
    # N·∫øu kh√¥ng c√≥ tag, t√¨m d√≤ng ch·ª©a "task" ho·∫∑c n·ªôi dung ƒë∆°n gi·∫£n
    lines = text.split('\n')
    for line in lines:
        if line.strip() and not line.strip().startswith('#') and 'task' not in line.lower():
            return line.strip()
    
    return ""


def _create_xml_tool_call_response(model: str, stream: bool = False) -> dict:
    """
    T·∫°o response v·ªõi XML tool calls cho Cline (Cline c·∫ßn XML trong content)
    H·ªó tr·ª£ c·∫£ streaming v√† non-streaming
    """
    xml_content = """<thinking>
I need to examine the current files to understand the task and create a plan. Let me start by reading the main application file to see the project structure.
</thinking>

<read_file>
<path>main.py</path>
<task_progress>
- [ ] Analyze project structure and main entry point
- [ ] Examine API routes and request handling
- [ ] Review response parsing logic
- [ ] Understand provider LLM interaction flow
</task_progress>
</read_file>"""

    if stream:
        return _create_streaming_response(model, xml_content)
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": xml_content,
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": 285,
                "completion_tokens": _count_tokens(xml_content),
                "total_tokens": 285 + _count_tokens(xml_content)
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }

def _create_standard_response(model: str, stream: bool = False) -> dict:
    """
    T·∫°o response ti√™u chu·∫©n cho Cline (c·∫ßn XML format)
    H·ªó tr·ª£ c·∫£ streaming v√† non-streaming
    """
    xml_content = """<thinking>
The user is interacting with the ZenEnd backend project. I should provide a helpful response and offer to explore the project structure.
</thinking>

<attempt_completion>
<result>Hello! I can see you're working with the ZenEnd backend project. I notice this is a FastAPI application with WebSocket support for bridging with ZenTab extension. The project structure includes API routes, WebSocket handlers, and core management components. How can I help you with this project?</result>
<task_progress>
- [x] Initialize conversation with user
- [ ] Wait for user's specific requirements
</task_progress>
</attempt_completion>"""

    if stream:
        return _create_streaming_response(model, xml_content)
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": xml_content,
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": 285,
                "completion_tokens": _count_tokens(xml_content),
                "total_tokens": 285 + _count_tokens(xml_content)
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }

def _create_streaming_response(model: str, content: str) -> dict:
    """
    T·∫°o streaming response ƒë∆°n gi·∫£n - ch·ªâ 1 chunk v·ªõi to√†n b·ªô content
    üÜï NOTE: Tr·∫£ v·ªÅ chunk format cho streaming, s·∫Ω ƒë∆∞·ª£c convert ·ªü routes.py n·∫øu c·∫ßn
    """
    response_id = f"chatcmpl-{uuid.uuid4().hex[:16]}"
    created_time = int(time.time())
    
    # Chunk duy nh·∫•t v·ªõi to√†n b·ªô content
    chunk = {
        "id": response_id,
        "object": "chat.completion.chunk",
        "created": created_time,
        "model": model,
        "choices": [{
            "index": 0,
            "delta": {
                "role": "assistant",
                "content": content
            },
            "finish_reason": "stop",
            "logprobs": None
        }],
        "usage": {
            "prompt_tokens": 285,
            "completion_tokens": _count_tokens(content),
            "total_tokens": 285 + _count_tokens(content)
        },
        "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
    }
    
    return chunk

def create_fake_response_with_multiple_tools(model: str = "deepseek-chat", stream: bool = False) -> dict:
    """
    T·∫°o fake response v·ªõi multiple tool calls
    """
    xml_content = """I'll help you explore the ZenEnd backend project. Let me start by examining the main application structure and API routes.

<read_file>
<path>main.py</path>
</read_file>

<read_file>
<path>api/routes.py</path>
</read_file>

<task_progress>
- [ ] Analyze main application structure
- [ ] Review API routes implementation
- [ ] Understand WebSocket handlers
- [ ] Test backend functionality
</task_progress>"""

    if stream:
        return _create_streaming_response(model, xml_content)
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": xml_content,
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": 285,
                "completion_tokens": _count_tokens(xml_content),
                "total_tokens": 285 + _count_tokens(xml_content)
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }


def create_fake_response_with_execute_command(model: str = "deepseek-chat", stream: bool = False) -> dict:
    """
    T·∫°o fake response v·ªõi execute command
    """
    xml_content = """Let me check if the backend server is running and test the API.

<execute_command>
<command>python main.py</command>
<requires_approval>false</requires_approval>
</execute_command>

<task_progress>
- [ ] Start backend server
- [ ] Test API endpoints
- [ ] Verify WebSocket connection
- [ ] Validate response format
</task_progress>"""

    if stream:
        return _create_streaming_response(model, xml_content)
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": xml_content,
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": 285,
                "completion_tokens": _count_tokens(xml_content),
                "total_tokens": 285 + _count_tokens(xml_content)
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }


def create_fake_response_with_list_files(model: str = "deepseek-chat", stream: bool = False) -> dict:
    """
    T·∫°o fake response v·ªõi list files
    """
    xml_content = """Let me first explore the project structure to understand the codebase better.

<list_files>
<path>.</path>
<recursive>true</recursive>
</list_files>

<task_progress>
- [ ] Explore project structure
- [ ] Identify key components
- [ ] Analyze code organization
- [ ] Understand dependencies
</task_progress>"""

    if stream:
        return _create_streaming_response(model, xml_content)
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": xml_content,
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": 285,
                "completion_tokens": _count_tokens(xml_content),
                "total_tokens": 285 + _count_tokens(xml_content)
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }


def create_fake_response_with_read_file(model: str = "deepseek-chat", file_path: str = "main.py", stream: bool = False) -> dict:
    """
    T·∫°o fake response v·ªõi read file
    """
    xml_content = f"""Let me examine the {file_path} file to understand the implementation and how it handles API requests.

<read_file>
<path>{file_path}</path>
<task_progress>
- [ ] Analyze {file_path} structure and components
- [ ] Understand request processing flow
- [ ] Identify key functions and classes for API handling
- [ ] Review response generation logic
- [ ] Document integration with provider LLM
</task_progress>
</read_file>"""

    if stream:
        return _create_streaming_response(model, xml_content)
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": xml_content,
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": 285,
                "completion_tokens": _count_tokens(xml_content),
                "total_tokens": 285 + _count_tokens(xml_content)
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }


def create_fake_error_response(model: str = "deepseek-chat", error_msg: str = "Test error", stream: bool = False) -> dict:
    """
    T·∫°o fake error response cho testing
    """
    if stream:
        return _create_streaming_response(model, f"Error: {error_msg}")
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": f"Error: {error_msg}",
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": 285,
                "completion_tokens": 15,
                "total_tokens": 300
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }


def create_fake_task_response(model: str = "deepseek-chat", task_description: str = "", stream: bool = False) -> dict:
    """
    T·∫°o fake response c·ª• th·ªÉ cho task
    """
    if not task_description:
        task_description = "the requested task"
    
    xml_content = f"""I understand you want me to work on: {task_description}

Let me start by exploring the project structure to understand the current implementation.

<list_files>
<path>.</path>
<recursive>true</recursive>
<task_progress>
- [ ] Analyze task requirements: {task_description}
- [ ] Explore project structure
- [ ] Identify relevant files and components
- [ ] Develop implementation strategy
</task_progress>
</list_files>"""

    if stream:
        return _create_streaming_response(model, xml_content)
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": xml_content,
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": 285,
                "completion_tokens": _count_tokens(xml_content),
                "total_tokens": 285 + _count_tokens(xml_content)
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }


def create_fake_response_with_specific_task(model: str = "deepseek-chat", task_type: str = "explore", stream: bool = False) -> dict:
    """
    T·∫°o fake response cho lo·∫°i task c·ª• th·ªÉ
    """
    task_responses = {
        "explore": create_fake_response_with_list_files,
        "api": lambda m, s: create_fake_response_with_read_file(m, "api/routes.py", s),
        "websocket": lambda m, s: create_fake_response_with_read_file(m, "websocket/server.py", s),
        "fake": lambda m, s: create_fake_response_with_read_file(m, "api/fake_response.py", s),
        "main": lambda m, s: create_fake_response_with_read_file(m, "main.py", s)
    }
    
    return task_responses.get(task_type, lambda m, s: create_fake_task_response(m, "", s))(model, stream)


def _calculate_tokens(messages: List[Any]) -> int:
    """T√≠nh to√°n s·ªë tokens t·ª´ messages (∆∞·ªõc l∆∞·ª£ng)"""
    if not messages:
        return 285
    
    total_tokens = 0
    for msg in messages:
        # X·ª≠ l√Ω c·∫£ object v√† dict
        if hasattr(msg, 'content'):
            content = msg.content
        elif hasattr(msg, 'get'):
            content = msg.get('content', '')
        else:
            continue
        
        if isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and item.get('type') == 'text':
                    total_tokens += _count_tokens(item.get('text', ''))
        elif isinstance(content, str):
            total_tokens += _count_tokens(content)
    
    return max(total_tokens, 285)  # Minimum token count

def _count_tokens(text: str) -> int:
    """ƒê·∫øm s·ªë tokens trong text (∆∞·ªõc l∆∞·ª£ng)"""
    # ∆Ø·ªõc l∆∞·ª£ng ƒë∆°n gi·∫£n: 1 token ~ 4 characters
    return max(len(text) // 4, 1)


def is_fake_mode_enabled() -> bool:
    return False

# C√°c h√†m ti·ªán √≠ch ƒë·ªÉ t·∫°o response c·ª• th·ªÉ
def get_response_for_testing(stream: bool = False) -> dict:
    """Tr·∫£ v·ªÅ response m·∫´u cho testing Cline integration"""
    return create_fake_response_with_list_files("deepseek-chat", stream)


def get_response_for_specific_tool(tool_name: str, stream: bool = False) -> dict:
    """Tr·∫£ v·ªÅ response v·ªõi tool c·ª• th·ªÉ"""
    tool_responses = {
        "read_file": create_fake_response_with_multiple_tools,
        "execute_command": create_fake_response_with_execute_command,
        "list_files": create_fake_response_with_list_files,
        "task": create_fake_task_response
    }
    
    return tool_responses.get(tool_name, create_fake_response)(stream=stream)


def get_response_for_task(task_description: str = "", stream: bool = False) -> dict:
    """Tr·∫£ v·ªÅ response cho task c·ª• th·ªÉ"""
    return create_fake_task_response("deepseek-chat", task_description, stream)


def get_response_for_task_type(task_type: str = "explore", stream: bool = False) -> dict:
    """Tr·∫£ v·ªÅ response cho lo·∫°i task c·ª• th·ªÉ"""
    return create_fake_response_with_specific_task("deepseek-chat", task_type, stream)


# Response templates for common scenarios
RESPONSE_TEMPLATES = {
    "simple_task": {
        "content": """<thinking>
User sent a simple task. I'll respond with attempt_completion.
</thinking>

<attempt_completion>
<result>I've received your task. I'm currently in fake response mode and ready to assist you with the ZenEnd project.</result>
<task_progress>
- [x] Acknowledge user task
- [ ] Wait for specific requirements
</task_progress>
</attempt_completion>""",
        "usage": {"prompt_tokens": 285, "completion_tokens": 85, "total_tokens": 370}
    },
    "project_exploration": {
        "content": """I'll help you explore the ZenEnd backend project. Let me start by examining the project structure.

<list_files>
<path>.</path>
<recursive>true</recursive>
<task_progress>
- [ ] Explore project structure
- [ ] Identify key components
- [ ] Analyze code organization
- [ ] Understand dependencies
</task_progress>
</list_files>""",
        "usage": {"prompt_tokens": 285, "completion_tokens": 72, "total_tokens": 357}
    },
    "api_analysis": {
        "content": """I'll analyze the API routes implementation.

<read_file>
<path>api/routes.py</path>
<task_progress>
- [ ] Analyze API routes structure
- [ ] Review request handling
- [ ] Check response formatting
- [ ] Test endpoint functionality
</task_progress>
</read_file>""",
        "usage": {"prompt_tokens": 285, "completion_tokens": 68, "total_tokens": 353}
    },
    "websocket_analysis": {
        "content": """I'll examine the WebSocket implementation.

<read_file>
<path>websocket/server.py</path>
<task_progress>
- [ ] Analyze WebSocket server
- [ ] Review connection handling
- [ ] Check message processing
- [ ] Test real-time functionality
</task_progress>
</read_file>""",
        "usage": {"prompt_tokens": 285, "completion_tokens": 70, "total_tokens": 355}
    }
}

def create_fake_attempt_completion_response(model: str = "deepseek-chat", result: str = "", task_progress: str = "", stream: bool = False) -> dict:
    if not result:
        result = "I've completed your task. I'm currently in fake response mode and ready to assist you with the ZenEnd project."
    
    if not task_progress:
        task_progress = """- [x] Receive and acknowledge user task
- [x] Process task
- [ ] Wait for next specific requirements"""

    xml_content = f"""<thinking>
I'm completing the user's task with the provided result and task progress.
</thinking>

<attempt_completion>
<result>{result}</result>
<task_progress>
{task_progress}
</task_progress>
</attempt_completion>"""

    if stream:
        return _create_streaming_response(model, xml_content)
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": xml_content,
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": 285,
                "completion_tokens": _count_tokens(xml_content),
                "total_tokens": 285 + _count_tokens(xml_content)
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }

def create_response_from_template(template_name: str, model: str = "deepseek-chat", stream: bool = False) -> dict:
    """T·∫°o response t·ª´ template"""
    template = RESPONSE_TEMPLATES.get(template_name, RESPONSE_TEMPLATES["project_exploration"])
    
    if stream:
        return _create_streaming_response(model, template["content"])
    else:
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": template["content"],
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": template["usage"],
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }

def _create_plan_mode_response(model: str, messages: List[Any], stream: bool = False) -> dict:
    """
    T·∫°o response cho PLAN MODE - kh√¥ng d√πng XML tool calls
    """
    plan_response = "<focus_chain id=\"fc-20251121-01\">\n  <task id=\"task-1\" title=\"Refactor WebSocket layer\">T·ªëi ∆∞u h√≥a v√† t√°ch WebSocket logic th√†nh service ri√™ng.</task>\n\n  <steps>\n    <step id=\"1\" status=\"todo\" focus=\"true\">\n      <title>Ph√¢n t√≠ch hi·ªán tr·∫°ng</title>\n      <instruction>ƒê·ªçc c√°c file li√™n quan: ws-manager.js, background.js, socket-controller. Li·ªát k√™ coupling v√† ƒëi·ªÉm c·∫ßn t√°ch.</instruction>\n      <output></output>\n    </step>\n\n    <step id=\"2\" status=\"todo\" focus=\"false\">\n      <title>Thi·∫øt k·∫ø Service API</title>\n      <instruction>ƒê·ªãnh nghƒ©a interface cho WebSocketService (connect, send, on, off, reconnect). Vi·∫øt spec ng·∫Øn.</instruction>\n      <output></output>\n    </step>\n\n    <step id=\"3\" status=\"todo\" focus=\"false\">\n      <title>T√°ch logic</title>\n      <instruction>Di chuy·ªÉn logic k·∫øt n·ªëi/queue/reconnect v√†o WebSocketService, gi·ªØ controller nh·∫π.</instruction>\n      <output></output>\n    </step>\n\n    <step id=\"4\" status=\"todo\" focus=\"false\">\n      <title>Refactor controllers</title>\n      <instruction>Update background v√† content scripts ƒë·ªÉ d√πng WebSocketService; th√™m logging v√† error handling.</instruction>\n      <output></output>\n    </step>\n\n    <step id=\"5\" status=\"todo\" focus=\"false\">\n      <title>Ki·ªÉm th·ª≠ end-to-end</title>\n      <instruction>Ch·∫°y scenario: connect, send/receive, reconnect; ghi l·ªói v√† s·ª≠a.</instruction>\n      <output></output>\n    </step>\n  </steps>\n\n  <analysis>∆Øu ti√™n ph√¢n t√≠ch k·ªπ file ƒë·ªÉ tr√°nh breaking change; step 1 ph·∫£i ho√†n th√†nh tr∆∞·ªõc khi t√°ch.</analysis>\n  <result></result>\n  <reflection></reflection>\n</focus_chain>"
    if stream:
        # Streaming response cho PLAN MODE
        return _create_streaming_response(model, plan_response)
    else:
        # Non-streaming response cho PLAN MODE  
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": plan_response,
                    "tool_calls": None
                },
                "finish_reason": "stop",
                "logprobs": None
            }],
            "usage": {
                "prompt_tokens": _calculate_tokens(messages) if messages else 285,
                "completion_tokens": _count_tokens(plan_response),
                "total_tokens": (_calculate_tokens(messages) if messages else 285) + _count_tokens(plan_response)
            },
            "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
        }