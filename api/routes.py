import re
import asyncio
from api.fake_response import create_fake_response
import time
import uuid
import json
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from typing import Optional

from fastapi import APIRouter, HTTPException, Depends, Header

def _extract_folder_path(messages: list) -> str | None:
    """
    Extract folder_path tá»« system/user messages.
    TÃ¬m pattern: # Current Working Directory (/path/to/folder)
    """
    for msg in messages:
        content = msg.content
        if isinstance(content, str):
            match = re.search(
                r'# Current Working Directory \(([^)]+)\)',
                content
            )
            if match:
                folder_path = match.group(1)
                return folder_path
        elif isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and item.get("type") == "text":
                    text = item.get("text", "")
                    match = re.search(
                        r'# Current Working Directory \(([^)]+)\)',
                        text
                    )
                    if match:
                        folder_path = match.group(1)
                        return folder_path
    return None

def _detect_new_task(messages: list) -> bool:
    """
    Kiá»ƒm tra xem cÃ³ <task></task> KHÃ”NG Rá»–NG trong MESSAGE CUá»I CÃ™NG khÃ´ng.
    Tráº£ vá» True náº¿u Ä‘Ã¢y lÃ  task má»›i (cÃ³ ná»™i dung task).
    """
    if not messages:
        print("[Request Detection] â„¹ï¸ No messages found - not a new task")
        return False
    
    latest_msg = messages[-1]
    content = latest_msg.content
    
    # Log message role vÃ  type
    msg_role = latest_msg.role
    print(f"[Request Detection] ğŸ” Checking latest message - Role: {msg_role}")
    
    if isinstance(content, str):
        has_task_tag = '<task>' in content and '</task>' in content
        if has_task_tag:
            # ğŸ†• CRITICAL: Check if task content is NOT empty
            task_start_idx = content.find('<task>') + 6
            task_end_idx = content.find('</task>')
            task_content = content[task_start_idx:task_end_idx].strip()
            
            if len(task_content) == 0:
                print(f"[Request Detection] âš ï¸ EMPTY <task></task> tag found - treating as EXISTING TASK")
                print(f"[Request Detection] ğŸ”„ CONTINUING EXISTING TASK")
                return False
            
            # Extract task content preview
            task_preview_end = min(task_end_idx + 7, len(content))
            task_preview = content[content.find('<task>'):task_preview_end][:100]
            print(f"[Request Detection] âœ… NEW TASK DETECTED (string content)")
            print(f"[Request Detection] ğŸ“‹ Task preview: {task_preview}...")
            return True
        else:
            print(f"[Request Detection] âŒ No <task> tag found (string content, {len(content)} chars)")
    elif isinstance(content, list):
        print(f"[Request Detection] ğŸ” Checking array content ({len(content)} items)")
        for idx, item in enumerate(content):
            if isinstance(item, dict) and item.get("type") == "text":
                text = item.get("text", "")
                has_task_tag = '<task>' in text and '</task>' in text
                if has_task_tag:
                    # ğŸ†• CRITICAL: Check if task content is NOT empty
                    task_start_idx = text.find('<task>') + 6
                    task_end_idx = text.find('</task>')
                    task_content = text[task_start_idx:task_end_idx].strip()
                    
                    if len(task_content) == 0:
                        print(f"[Request Detection] âš ï¸ EMPTY <task></task> tag found in array item {idx} - treating as EXISTING TASK")
                        print(f"[Request Detection] ğŸ”„ CONTINUING EXISTING TASK")
                        return False
                    
                    # Extract task content preview
                    task_preview_end = min(task_end_idx + 7, len(text))
                    task_preview = text[text.find('<task>'):task_preview_end][:100]
                    print(f"[Request Detection] âœ… NEW TASK DETECTED (array item {idx})")
                    print(f"[Request Detection] ğŸ“‹ Task preview: {task_preview}...")
                    return True
        print(f"[Request Detection] âŒ No <task> tag found in any array items")
    else:
        print(f"[Request Detection] âš ï¸ Unknown content type: {type(content)}")
    
    print("[Request Detection] ğŸ”„ CONTINUING EXISTING TASK")
    return False

def _validate_and_fix_response(response: dict, request_id: str, is_fake: bool = False) -> dict:
    """
    Rebuild response tá»« ZenTab thÃ nh clean OpenAI completion format.
    Extract tá»«ng field vÃ  táº¡o láº¡i object má»›i thay vÃ¬ parse phá»©c táº¡p.
    """
    if not isinstance(response, dict):
        raise HTTPException(status_code=500, detail="Invalid response format: not a dict")
    
    # Extract top-level fields
    response_id = response.get('id', f'chatcmpl-{request_id}')
    object_type = response.get('object', 'chat.completion.chunk')
    created = response.get('created', int(time.time()))
    model = response.get('model', 'deepseek-chat')
    system_fingerprint = response.get('system_fingerprint', f'fp_{uuid.uuid4().hex[:8]}')
    
    # Extract choices array
    original_choices = response.get('choices', [])
    if not original_choices or len(original_choices) == 0:
        raise HTTPException(status_code=500, detail="Invalid response: no choices")
    
    # Extract first choice
    original_choice = original_choices[0]
    choice_index = original_choice.get('index', 0)
    finish_reason = original_choice.get('finish_reason', 'stop')
    logprobs = original_choice.get('logprobs', None)
    
    # Extract message/delta
    message_data = original_choice.get('message') or original_choice.get('delta', {})
    role = message_data.get('role', 'assistant')
    raw_content = message_data.get('content', '')
    tool_calls = message_data.get('tool_calls', None)
    
    # Fix: Decode escaped newlines
    content = raw_content
    if isinstance(raw_content, str) and '\\n' in raw_content:
        content = raw_content.replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t')
    
    # Extract usage
    original_usage = response.get('usage', {})
    prompt_tokens = original_usage.get('prompt_tokens', 0)
    completion_tokens = original_usage.get('completion_tokens', 0)
    total_tokens = original_usage.get('total_tokens', 0)
    
    # Rebuild: Fake dÃ¹ng 'message' + 'tool_calls', Real dÃ¹ng 'delta'
    if is_fake:
        message_obj = {
            'role': role,
            'content': content
        }
        if tool_calls and isinstance(tool_calls, list) and len(tool_calls) > 0:
            message_obj['tool_calls'] = tool_calls
        
        choice_data = {
            'index': choice_index,
            'message': message_obj,
            'finish_reason': finish_reason,
            'logprobs': logprobs
        }
    else:
        choice_data = {
            'index': choice_index,
            'delta': {
                'role': role,
                'content': content
            },
            'finish_reason': finish_reason,
            'logprobs': logprobs
        }
    
    # Rebuild clean response
    clean_response = {
        'id': response_id,
        'object': object_type,
        'created': created,
        'model': model,
        'choices': [choice_data],
        'usage': {
            'prompt_tokens': prompt_tokens,
            'completion_tokens': completion_tokens,
            'total_tokens': total_tokens
        },
        'system_fingerprint': system_fingerprint
    }
    
    return clean_response

from config.settings import REQUEST_TIMEOUT
from models import ChatCompletionRequest
from .dependencies import verify_api_key
import uuid
import time

router = APIRouter()

def setup_routes(app, port_manager):
    """Setup routes vá»›i port_manager dependency"""
    
    from fastapi.responses import JSONResponse
    
    @router.get("/v1/model/info")
    async def model_info(api_key: str = Depends(verify_api_key)):
        """
        Tráº£ vá» thÃ´ng tin model cho Cline extension
        """
        return {
            "id": "deepseek-chat",
            "object": "model",
            "created": 1234567890,
            "owned_by": "zenend",
            "permission": [],
            "root": "deepseek-chat",
            "parent": None,
            "description": "DeepSeek Chat via ZenTab extension"
        }

    @router.get("/v1/models")
    async def list_models(api_key: str = Depends(verify_api_key)):
        """
        Danh sÃ¡ch models (OpenAI compatible)
        """
        return {
            "object": "list",
            "data": [
                {
                    "id": "deepseek-chat",
                    "object": "model",
                    "created": 1234567890,
                    "owned_by": "zenend"
                },
                {
                    "id": "deepseek-coder",
                    "object": "model",
                    "created": 1234567890,
                    "owned_by": "zenend"
                }
            ]
        }

    @router.post("/v1/chat/completions")
    async def chat_completions(
        request: ChatCompletionRequest,
        api_key: str = Depends(verify_api_key)
    ):
        from api.fake_response import is_fake_mode_enabled, create_fake_response
        from fastapi.responses import StreamingResponse
        
        if is_fake_mode_enabled():
            fake_response = create_fake_response(
                model=request.model, 
                messages=request.messages,
                stream=request.stream if hasattr(request, 'stream') else False
            )
            
            fake_request_id = uuid.uuid4().hex[:16]
            
            if request.stream and fake_response.get("object") == "chat.completion.chunk":
                async def generate():
                    yield f"data: {json.dumps(fake_response)}\n\n"
                    yield "data: [DONE]\n\n"
                
                return StreamingResponse(
                    generate(),
                    media_type="text/event-stream"
                )
            else:
                try:
                    fake_response = _validate_and_fix_response(
                        fake_response, 
                        request_id=f"fake-{fake_request_id}", 
                        is_fake=True
                    )
                except Exception as e:
                    fake_response = {
                        "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
                        "object": "chat.completion",
                        "created": int(time.time()),
                        "model": request.model,
                        "choices": [{
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": "Fake response validation error"
                            },
                            "finish_reason": "stop",
                            "logprobs": None
                        }],
                        "usage": {
                            "prompt_tokens": 0,
                            "completion_tokens": 0,
                            "total_tokens": 0
                        },
                        "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
                    }
                
                return fake_response

        SUPPORTED_MODELS = ["deepseek-chat", "deepseek-coder", "deepseek-coder-v2"]
        if request.model not in SUPPORTED_MODELS:
            raise HTTPException(
                status_code=400, 
                detail=f"Model '{request.model}' not supported. Available models: {', '.join(SUPPORTED_MODELS)}"
            )
        
        conn_status = port_manager.get_connection_status()
        
        print(f"[Chat Completion] ğŸ”Œ Connection Status Check:")
        print(f"[Chat Completion]    - websocket_connected: {conn_status.get('websocket_connected')}")
        print(f"[Chat Completion]    - websocket_open: {conn_status.get('websocket_open')}")
        print(f"[Chat Completion]    - Full status: {conn_status}")
        
        if not conn_status.get('websocket_connected') or not conn_status.get('websocket_open'):
            print(f"[Chat Completion] âŒ 503 ERROR: WebSocket not connected")
            raise HTTPException(
                status_code=503,
                detail="WebSocket not connected. Please ensure ZenTab extension is connected to backend."
            )
        
        # Extract folder_path vÃ  detect new task
        folder_path = _extract_folder_path(request.messages)
        is_new_task = _detect_new_task(request.messages)

        # Chá»n tab dá»±a trÃªn new task vs existing task
        if is_new_task:
            # New task: request táº¥t cáº£ tabs ráº£nh
            print(f"[Chat Completion] ğŸ†• NEW TASK - Requesting fresh tabs...")
            available_tabs = await port_manager.request_fresh_tabs(timeout=10.0)
            
            print(f"[Chat Completion] ğŸ“‹ Fresh tabs response: {available_tabs}")
            print(f"[Chat Completion] ğŸ“‹ Tabs count: {len(available_tabs) if available_tabs else 0}")
            
            if not available_tabs or len(available_tabs) == 0:
                print(f"[Chat Completion] âŒ 503 ERROR: No fresh tabs available for NEW TASK")
                raise HTTPException(
                    status_code=503,
                    detail="No tabs available. Please open DeepSeek tabs in ZenTab extension first."
                )
            
            selected_tab = available_tabs[0]
            print(f"[Chat Completion] âœ… Selected fresh tab: {selected_tab}")
        else:
            # Existing task: tÃ¬m tab cÃ³ folder_path khá»›p
            print(f"[Chat Completion] ğŸ”„ EXISTING TASK - Looking for tabs by folder...")
            print(f"[Chat Completion] ğŸ“ Folder path extracted: {folder_path}")
            
            if not folder_path:
                print(f"[Chat Completion] âŒ 400 ERROR: No folder_path found in request")
                raise HTTPException(
                    status_code=400,
                    detail="Cannot find folder_path in request. Please ensure the task context is included."
                )
            
            folder_tabs = await port_manager.request_tabs_by_folder(folder_path, timeout=10.0)
            
            print(f"[Chat Completion] ğŸ“‹ Folder tabs response: {folder_tabs}")
            print(f"[Chat Completion] ğŸ“‹ Tabs count: {len(folder_tabs) if folder_tabs else 0}")
            
            if not folder_tabs or len(folder_tabs) == 0:
                print(f"[Chat Completion] âŒ 503 ERROR: No tabs linked to folder '{folder_path}'")
                raise HTTPException(
                    status_code=503,
                    detail=f"No tabs linked to folder '{folder_path}'. Please start a new task first."
                )
            
            selected_tab = folder_tabs[0]
            print(f"[Chat Completion] âœ… Selected folder tab: {selected_tab}")
        
        tab_id = selected_tab.get('tabId')
        
        # ğŸ†• LOG: Tab selection result
        print(f"[Chat Completion] âœ… TAB SELECTED")
        print(f"[Chat Completion] ğŸ”¢ Tab ID: {tab_id}")
        print(f"[Chat Completion] ğŸ“Š Tab Status: {selected_tab.get('status', 'unknown')}")
        print(f"[Chat Completion] âœ”ï¸ Can Accept: {selected_tab.get('canAccept', False)}")
        if selected_tab.get('folderPath'):
            print(f"[Chat Completion] ğŸ“ Linked Folder: {selected_tab.get('folderPath')}")
        else:
            print(f"[Chat Completion] ğŸ“ Linked Folder: None (will be linked after prompt sent)")
        
        if not tab_id or not isinstance(tab_id, int) or tab_id <= 0:
            raise HTTPException(
                status_code=500,
                detail=f"Invalid tab ID received from ZenTab: {tab_id}"
            )
        
        tab_status = selected_tab.get('status', 'unknown')
        can_accept = selected_tab.get('canAccept', False)
        
        print(f"[Chat Completion] ğŸ” Tab validation:")
        print(f"[Chat Completion]    - tab_status: {tab_status}")
        print(f"[Chat Completion]    - can_accept: {can_accept}")
        
        if tab_status != 'free' or not can_accept:
            print(f"[Chat Completion] âŒ 503 ERROR: Tab not ready - status={tab_status}, canAccept={can_accept}")
            raise HTTPException(
                status_code=503,
                detail=f"Tab is not ready to accept requests. Status: {tab_status}, Can accept: {can_accept}"
            )

        request_id = f"api-{uuid.uuid4().hex[:16]}"

        # Extract BOTH system prompt and user messages
        system_messages = [msg for msg in request.messages if msg.role == "system"]
        user_messages = [msg for msg in request.messages if msg.role == "user"]
        
        if not user_messages:
            raise HTTPException(status_code=400, detail="No user message found in request")

        # ğŸ†• CRITICAL: Chá»‰ extract system prompt KHI LÃ€ NEW TASK
        # Existing task khÃ´ng cáº§n system prompt vÃ¬ DeepSeek Ä‘Ã£ cÃ³ context
        system_prompt = ""
        if is_new_task and system_messages:
            system_content = system_messages[0].content
            if isinstance(system_content, str):
                system_prompt = system_content
            elif isinstance(system_content, list):
                text_parts = []
                for item in system_content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_parts.append(item.get("text", ""))
                system_prompt = "\n\n".join(text_parts)
            
            print(f"[Chat Completion] ğŸ“‹ System Prompt: Included (NEW TASK - {len(system_prompt)} chars)")
        else:
            print(f"[Chat Completion] ğŸ“‹ System Prompt: Skipped (EXISTING TASK - using conversation context)")
        
        # Extract user message
        raw_content = user_messages[-1].content
        
        if isinstance(raw_content, list):
            text_parts = []
            for item in raw_content:
                if isinstance(item, dict):
                    if item.get("type") == "text":
                        text_parts.append(item.get("text", ""))
                    elif item.get("type") == "image":
                        text_parts.append("[IMAGE - Not supported]")
            
            user_prompt = "\n\n".join(text_parts)
        else:
            user_prompt = raw_content
        
        # ğŸ†• LOG: User prompt analysis
        print("\n" + "~"*80)
        print("[User Prompt Analysis] ğŸ“ USER PROMPT DETAILS")
        print("~"*80)
        print(f"[User Prompt Analysis] ğŸ“ Total Length: {len(user_prompt)} chars")
        print(f"[User Prompt Analysis] ğŸ” Contains <task> tag: {'<task>' in user_prompt}")
        print(f"[User Prompt Analysis] ğŸ” Contains </task> tag: {'</task>' in user_prompt}")
        
        # Check if task tag is empty
        if '<task>' in user_prompt and '</task>' in user_prompt:
            task_start = user_prompt.find('<task>') + 6
            task_end = user_prompt.find('</task>')
            task_content = user_prompt[task_start:task_end].strip()
            print(f"[User Prompt Analysis] ğŸ“¦ Task Content Length: {len(task_content)} chars")
            print(f"[User Prompt Analysis] ğŸ“¦ Task Is Empty: {len(task_content) == 0}")
            if len(task_content) > 0:
                preview = task_content[:200].replace('\n', ' ')
                print(f"[User Prompt Analysis] ğŸ“‹ Task Preview: {preview}...")
            else:
                print(f"[User Prompt Analysis] âš ï¸ Task tag is EMPTY!")
        
        # Show first 500 chars of user prompt
        prompt_preview = user_prompt[:500].replace('\n', ' ')
        print(f"[User Prompt Analysis] ğŸ“„ Prompt Preview (500 chars): {prompt_preview}...")
        print("~"*80 + "\n")

        port_manager.request_to_tab[request_id] = tab_id
        
        # ğŸ†• LOG: Sending prompt details
        print("\n" + "-"*80)
        print(f"[Chat Completion] ğŸ“¤ SENDING PROMPT TO ZENTAB")
        print("-"*80)
        print(f"[Chat Completion] ğŸ†” Request ID: {request_id}")
        print(f"[Chat Completion] ğŸ”¢ Target Tab: {tab_id}")
        if is_new_task:
            print(f"[Chat Completion] ğŸ“ System Prompt: {len(system_prompt)} chars (NEW TASK)")
        else:
            print(f"[Chat Completion] ğŸ“ System Prompt: Skipped (EXISTING TASK)")
        print(f"[Chat Completion] ğŸ’¬ User Prompt: {len(user_prompt)} chars")
        print(f"[Chat Completion] ğŸ·ï¸ Is New Task: {is_new_task}")
        
        # Send prompt vá»›i folder_path handling
        ws_message = {
            "type": "sendPrompt",
            "tabId": tab_id,
            "systemPrompt": system_prompt,
            "userPrompt": user_prompt,
            "requestId": request_id,
            "isNewTask": is_new_task
        }
        
        # Chá»‰ gá»­i folder_path khi lÃ  new task
        if is_new_task and folder_path:
            ws_message["folderPath"] = folder_path
            print(f"[Chat Completion] ğŸ“ Folder Path (NEW TASK): {folder_path}")
            print(f"[Chat Completion] ğŸ”— Action: Tab will be linked to this folder after prompt sent")
        else:
            print(f"[Chat Completion] ğŸ“ Folder Path: Not included (existing task)")
        print("-"*80 + "\n")
        
        try:
            await port_manager.websocket.send(json.dumps(ws_message))
        except Exception as e:
            port_manager.request_to_tab.pop(request_id, None)
            raise HTTPException(status_code=500, detail=f"Failed to send prompt: {str(e)}")
        
        try:
            response = await port_manager.wait_for_response(request_id, REQUEST_TIMEOUT)
            
            if "error" in response:
                error_msg = response["error"]
                if "cooling down" in error_msg.lower() or "not ready" in error_msg.lower():
                    raise HTTPException(
                        status_code=503,
                        detail=error_msg
                    )
                else:
                    raise HTTPException(status_code=500, detail=error_msg)
            
            port_manager.mark_request_completed(request_id)

            asyncio.create_task(port_manager.schedule_request_cleanup(request_id, delay=30.0))
            
            response = _validate_and_fix_response(response, request_id, is_fake=False)
            
            # Wrap real response trong StreamingResponse
            if response.get("object") == "chat.completion.chunk":
                async def generate_real():
                    yield f"data: {json.dumps(response)}\n\n"
                    yield "data: [DONE]\n\n"
                
                return StreamingResponse(
                    generate_real(),
                    media_type="text/event-stream"
                )
            else:
                return response
            
        except HTTPException as he:
            port_manager.mark_request_processed(request_id)
            
            asyncio.create_task(port_manager.schedule_request_cleanup(request_id, delay=10.0))
            
            if he.status_code == 503:
                raise
            else:
                fallback_response = {
                    "id": f"chatcmpl-{uuid.uuid4().hex[:16]}",
                    "object": "chat.completion",
                    "created": int(time.time()),
                    "model": "deepseek-chat",
                    "choices": [{
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": "I apologize, but I'm experiencing temporary technical difficulties. Please try your request again in a moment."
                        },
                        "finish_reason": "stop",
                        "logprobs": None
                    }],
                    "usage": {
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0
                    },
                    "system_fingerprint": f"fp_{uuid.uuid4().hex[:8]}"
                }
                
                return fallback_response
        except Exception as e:
            port_manager.mark_request_processed(request_id)
            
            asyncio.create_task(port_manager.schedule_request_cleanup(request_id, delay=10.0))
            
            raise HTTPException(status_code=500, detail=str(e))
        finally:
            pass
    
    app.include_router(router)